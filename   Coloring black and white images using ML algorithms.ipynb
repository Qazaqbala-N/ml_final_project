{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                  Coloring black and white images using ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team Members and Contributions\n",
    "\n",
    "Our project was a collaborative effort, with each team member playing a crucial role in its successful completion. Here‚Äôs a breakdown of our contributions:\n",
    "\n",
    "- **Nukushev Daniyar**  \n",
    "  Focused on **data preparation**, including collecting and preprocessing images, converting them to grayscale, and normalizing pixel values for the models. Daniyar also ensured the dataset was clean and ready for training.\n",
    "\n",
    "- **Abishev Rauan**  \n",
    "  Specialized in **model development**, designing the architecture of the U-Net and GAN models. Rauan worked on fine-tuning hyperparameters and implementing the training pipeline to achieve optimal performance.\n",
    "\n",
    "- **Momynkul Nurzhan**  \n",
    "  Handled **evaluation and visualization**. Nurzhan analyzed the model outputs using quantitative metrics and visual quality checks, creating insightful visualizations for comparison between the models.\n",
    "\n",
    "- **Melissov Nurdaulet**  \n",
    "  Oversaw **project management and integration**, ensuring seamless collaboration between team members. Nurdaulet also contributed to report writing, presentation design, and deploying the prototype for demonstration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains the steps taken to develop a machine learning model that colorizes black and white images. Our approach uses U-Net and GAN models for this task. The project was completed by a team of four members, each contributing to different aspects like data preparation, model development, and evaluation. I'll assume you have basic knowledge about deep learning, GANs, and PyTorch library for the rest of the article. Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary goal is to restore colors in grayscale images using advanced machine learning techniques. This process is essential for tasks like reviving old memories through photos or adding color to creative works in a more automated and efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Lab Color Space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are often represented in the RGB color space, where each pixel has values for red, green, and blue channels. However, for this project, We chose the Lab color space. In this space:\n",
    "\n",
    "The L channel represents lightness (grayscale intensity).\n",
    "The a and b channels represent color components (green-red and yellow-blue, respectively).\n",
    "\n",
    "![rgb image](./files/rgb.jpg) **RGB**\n",
    "\n",
    "Using Lab simplifies the task because the model can focus on predicting the two color channels (*a and *b) while taking the L channel (grayscale) as input. This approach reduces complexity compared to working directly with RGB values.\n",
    "\n",
    "![lab image](./files/lab.jpg) **L*A*B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Two types of losses were utilized to guide the model's learning:\n",
    "\n",
    "1. **L1 Loss**  \n",
    "   - A regression loss function used to minimize the difference between the predicted and actual colors in the image. This helps the model generate colorized images that are closer to the ground truth.\n",
    "\n",
    "2. **Adversarial Loss (GAN Loss)**  \n",
    "   - Used in the Generative Adversarial Network (GAN) to ensure the generated images appear realistic. The discriminator network classifies the images as \"real\" or \"fake,\" and the adversarial loss helps improve the quality of generated images by encouraging the generator to produce more realistic outputs.\n",
    "   \n",
    "   \n",
    "The generator in the GAN architecture predicts the color channels, while the discriminator evaluates their authenticity by comparing them against real images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Deeper Dive into GANs\n",
    "\n",
    "For this project, I designed a **conditional GAN**, combining it with an additional loss function, the **L1 loss**, to achieve high-quality results. Let‚Äôs break it down.\n",
    "\n",
    "#### GAN Components\n",
    "\n",
    "In a **Generative Adversarial Network (GAN)**, there are two primary components:\n",
    "\n",
    "1. **The Generator**  \n",
    "   The generator model produces data. In this case, it takes a grayscale (1-channel) image and generates a 2-channel output, corresponding to the *a* and *b* channels of the Lab color space.\n",
    "\n",
    "2. **The Discriminator**  \n",
    "   The discriminator model evaluates the authenticity of the generator's output. It takes the 2-channel output from the generator, concatenates it with the input grayscale image to form a 3-channel image, and determines whether the image is \"real\" or \"fake.\" The discriminator is trained using real 3-channel images from the dataset for comparison.\n",
    "\n",
    "#### Conditioning the GAN\n",
    "\n",
    "The conditioning in this GAN refers to the grayscale input image, which is provided to both the generator and discriminator. This ensures that both models are conditioned on the same input, allowing the generator to learn context-specific colorization.\n",
    "\n",
    "#### Mathematical Representation\n",
    "\n",
    "Let:\n",
    "\n",
    "- **ùë•** be the grayscale image (condition).\n",
    "- **ùëß** represent input noise (if applicable) for the generator.\n",
    "- **ùë¶** denote the desired 2-channel output from the generator (*a* and *b* channels of a real image).\n",
    "- **ùê∫** and **ùê∑** represent the generator and discriminator models, respectively.\n",
    "\n",
    "The GAN loss function ensures the generator produces outputs that the discriminator cannot distinguish from real images. The generator‚Äôs task is to minimize this loss, while the discriminator tries to maximize it. Additionally, the **L1 loss** helps align the predicted colors with the actual colors, ensuring both realism and accuracy.\n",
    "\n",
    "#### Incorporating Noise\n",
    "\n",
    "Instead of feeding random noise vectors (**ùëß**) directly to the generator, I introduced noise through dropout layers within the generator‚Äôs architecture. This method injects variability during training, improving robustness and ensuring better generalization in the generated outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Optimization\n",
    "\n",
    "To achieve visually appealing and realistic colorizations, I designed a combined loss function for the model. The initial GAN loss ensures the generator produces outputs that the discriminator considers realistic. However, to introduce more supervision and guide the model toward accurate color predictions, I incorporated **L1 loss** (also known as mean absolute error).\n",
    "\n",
    "### Role of L1 Loss\n",
    "\n",
    "L1 loss measures the difference between the predicted and actual color channels. While effective, using L1 loss alone leads to overly conservative results‚Äîcommonly gray or brown tones. This occurs because the model minimizes L1 loss by averaging colors when unsure, resulting in less vibrant outputs. Compared to L2 loss (mean squared error), L1 loss is better at reducing this \"grayish\" effect and produces more defined colors.\n",
    "![Alt Text](l1_loss.jpg)\n",
    "\n",
    "### Combined Loss Function\n",
    "\n",
    "The final loss function combines:\n",
    "1. **Adversarial Loss (GAN Loss)**: Encourages realistic outputs by fooling the discriminator.\n",
    "2. **L1 Loss**: Ensures the predicted colors closely match the ground truth.\n",
    "\n",
    "The combined loss function is defined as:\n",
    "\n",
    "![Alt Text](loss.jpg)\n",
    "\n",
    "\n",
    "Here, \\(\\lambda\\) is a balancing factor that determines the relative importance of the two loss terms. This approach allows the model to generate visually realistic outputs while maintaining high color accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "With the theory in place, the next step is implementation. I'll start with the baseline method and then introduce refinements to achieve significantly improved results in just a few hours of training on a smaller dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1‚Ää-‚ÄäImplementing the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading Image Paths\n",
    "\n",
    "For this project,We are using only 8,000 images from the COCO dataset that I had available on my device. This makes the training set size a fraction of what could be used in larger-scale projects.\n",
    "\n",
    "You can use almost any dataset for this task, as long as it contains a variety of scenes and locations, which will help the model learn how to colorize images effectively. For example, you could use ImageNet, but for this project, only 8,000 images would be needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_colab = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.x Preparing Colab for Running the Code\n",
    "\n",
    "If you are running this on Google Colab, you can uncomment and execute the following code to install the `fastai` library. Most of the code in this project uses pure PyTorch, but we need `fastai` for downloading part of the COCO dataset and for one other step in the second section.\n",
    "\n",
    "Also, make sure to set your Colab runtime to GPU to speed up the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastai==2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will download about 20,000 images from COCO dataset. Notice that **we are going to use only 8000 of them** for training. Also you can use any other dataset like ImageNet as long as it contains various scenes and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.data.external import untar_data, URLs\n",
    "# coco_path = untar_data(URLs.COCO_SAMPLE)\n",
    "# coco_path = str(coco_path) + \"/train_sample\"\n",
    "# use_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab == True:\n",
    "    path = coco_path\n",
    "else:\n",
    "    path = \"Your path to the dataset\"\n",
    "    \n",
    "paths = glob.glob(path + \"/*.jpg\") # Grabbing all the image file names\n",
    "np.random.seed(123)\n",
    "paths_subset = np.random.choice(paths, 10_000, replace=False) # choosing 1000 images randomly\n",
    "rand_idxs = np.random.permutation(10_000)\n",
    "train_idxs = rand_idxs[:8000] # choosing the first 8000 as training set\n",
    "val_idxs = rand_idxs[8000:] # choosing last 2000 as validation set\n",
    "train_paths = paths_subset[train_idxs]\n",
    "val_paths = paths_subset[val_idxs]\n",
    "print(len(train_paths), len(val_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "for ax, img_path in zip(axes.flatten(), train_paths):\n",
    "    ax.imshow(Image.open(img_path))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are using the same dataset and number of training samples, the exact 8000 images that you train your model on may vary (although we are seeding!) because the dataset here has only 20000 images with different ordering while we sampled 10000 images from the complete dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2- Making Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope the code is self-explanatory. The steps include resizing the images and applying horizontal flipping (only for the training set). Afterward, we read each image in RGB format, convert it to the Lab color space, and separate the first channel (grayscale) and the color channels. The grayscale channel becomes the input, and the color channels are used as the targets for the model. Finally, we create the data loaders for training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, paths, split='train'):\n",
    "        if split == 'train':\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.Resize((SIZE, SIZE),  Image.BICUBIC),\n",
    "                transforms.RandomHorizontalFlip(), # A little data augmentation!\n",
    "            ])\n",
    "        elif split == 'val':\n",
    "            self.transforms = transforms.Resize((SIZE, SIZE),  Image.BICUBIC)\n",
    "        \n",
    "        self.split = split\n",
    "        self.size = SIZE\n",
    "        self.paths = paths\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        img = self.transforms(img)\n",
    "        img = np.array(img)\n",
    "        img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b\n",
    "        img_lab = transforms.ToTensor()(img_lab)\n",
    "        L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n",
    "        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n",
    "        \n",
    "        return {'L': L, 'ab': ab}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "def make_dataloaders(batch_size=16, n_workers=4, pin_memory=True, **kwargs): # A handy function to make our dataloaders\n",
    "    dataset = ColorizationDataset(**kwargs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n",
    "                            pin_memory=pin_memory)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = make_dataloaders(paths=train_paths, split='train')\n",
    "val_dl = make_dataloaders(paths=val_paths, split='val')\n",
    "\n",
    "data = next(iter(train_dl))\n",
    "Ls, abs_ = data['L'], data['ab']\n",
    "print(Ls.shape, abs_.shape)\n",
    "print(len(train_dl), len(val_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Generator\n",
    "\n",
    "The generator model is based on U-Net, which is a bit complicated and requires explanation. The code implements a U-Net architecture to be used as the generator of our GAN. While the detailed workings of the code are beyond the scope here, the key idea is that the U-Net starts with the middle part of the architecture (the \"bottleneck\" of the U-shape) and progressively adds down-sampling and up-sampling modules on the left and right sides, respectively. This process continues until it reaches the input and output modules.\n",
    "\n",
    "To give a clearer understanding of what is happening in the code, consider the following illustration:\n",
    "\n",
    "![U-Net Diagram](unet)\n",
    "\n",
    "The blue rectangles in the diagram show the order in which the related modules are built with the code. While the U-Net we are implementing has more layers than shown here, this diagram gives a good sense of the structure. Notice that in the code, we go down 8 layers. So, if we start with a 256x256 image, by the time it reaches the middle of the U-Net, the image is reduced to a 1x1 (256 / 2‚Å∏) image, and then it is up-sampled back to a 256x256 image with two color channels. \n",
    "\n",
    "This code snippet is quite fascinating, and we highly recommend experimenting with it to fully understand how each part functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n",
    "                 innermost=False, outermost=False):\n",
    "        super().__init__()\n",
    "        self.outermost = outermost\n",
    "        if input_c is None: input_c = nf\n",
    "        downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=False)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = nn.BatchNorm2d(ni)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = nn.BatchNorm2d(nf)\n",
    "        \n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            if dropout: up += [nn.Dropout(0.5)]\n",
    "            model = down + [submodule] + up\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([x, self.model(x)], 1)\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n",
    "        super().__init__()\n",
    "        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n",
    "        for _ in range(n_down - 5):\n",
    "            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n",
    "        out_filters = num_filters * 8\n",
    "        for _ in range(3):\n",
    "            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n",
    "            out_filters //= 2\n",
    "        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Discriminator\n",
    "\n",
    "The architecture of our discriminator is quite straightforward. The code implements a model by stacking blocks of Conv-BatchNorm-LeakyReLU to determine whether the input image is real or fake. It is important to note that the first and last blocks do not use normalization, and the last block does not have an activation function. This is because the activation is embedded in the loss function that we will use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_c, num_filters=64, n_down=3):\n",
    "        super().__init__()\n",
    "        model = [self.get_layers(input_c, num_filters, norm=False)]\n",
    "        model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down-1) else 2) \n",
    "                          for i in range(n_down)] # the 'if' statement is taking care of not using\n",
    "                                                  # stride of 2 for the last block in this loop\n",
    "        model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)] # Make sure to not use normalization or\n",
    "                                                                                             # activation for the last layer of the model\n",
    "        self.model = nn.Sequential(*model)                                                   \n",
    "        \n",
    "    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True): # when needing to make some repeatitive blocks of layers,\n",
    "        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]          # it's always helpful to make a separate method for that purpose\n",
    "        if norm: layers += [nn.BatchNorm2d(nf)]\n",
    "        if act: layers += [nn.LeakyReLU(0.2, True)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at its blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PatchDiscriminator(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And its output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = PatchDiscriminator(3)\n",
    "dummy_input = torch.randn(16, 3, 256, 256) # batch_size, channels, size, size\n",
    "out = discriminator(dummy_input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a \"Patch\" Discriminator here. But what does that mean? In a vanilla discriminator, the model outputs a single number (a scalar) that indicates how real or fake the entire input image is. However, in a patch discriminator, the model outputs one number for every patch of the image (e.g., 70x70 pixels) and decides whether each patch is real or fake individually.\n",
    "\n",
    "Using a patch discriminator for the task of colorization seems reasonable because the model needs to make local changes, and considering the entire image at once might overlook important subtle details. Instead, by evaluating smaller patches, the model can focus on more localized information, which is crucial for accurate colorization.\n",
    "\n",
    "In this setup, the output shape of the model is 30x30, but this doesn‚Äôt mean the patches are 30x30. The actual patch size is determined by calculating the receptive field of each of the 900 (30x30) output values, which, in this case, corresponds to a patch size of 70x70 pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 GAN Loss\n",
    "\n",
    "The `GANLoss` class is used to calculate the GAN loss for the model. During initialization, the type of loss to be used (e.g., \"vanilla\" for Binary Cross-Entropy loss) is specified, and constant tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
    "        if gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "    \n",
    "    def get_labels(self, preds, target_is_real):\n",
    "        if target_is_real:\n",
    "            labels = self.real_label\n",
    "        else:\n",
    "            labels = self.fake_label\n",
    "        return labels.expand_as(preds)\n",
    "    \n",
    "    def __call__(self, preds, target_is_real):\n",
    "        labels = self.get_labels(preds, target_is_real)\n",
    "        loss = self.loss(preds, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.x Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initialization is a critical step in ensuring stable and effective training. The logic for initializing the model involves setting the weights with a mean of 0.0 and a standard deviation of 0.02, which are commonly used hyperparameters in deep learning projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init='norm', gain=0.02):\n",
    "    \n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
    "            if init == 'norm':\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
    "            elif init == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            \n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif 'BatchNorm2d' in classname:\n",
    "            nn.init.normal_(m.weight.data, 1., gain)\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "            \n",
    "    net.apply(init_func)\n",
    "    print(f\"model initialized with {init} initialization\")\n",
    "    return net\n",
    "\n",
    "def init_model(model, device):\n",
    "    model = model.to(device)\n",
    "    model = init_weights(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Everything Together\n",
    "\n",
    "This class integrates all the components and implements methods to handle the training of the complete model.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Initialization**:  \n",
    "   The generator and discriminator are defined using previously implemented functions and initialized using the `init_model` function. Additionally, two loss functions (adversarial loss and L1 loss) and optimizers for both the generator and discriminator are defined.\n",
    "\n",
    "2. **Optimize Method**:  \n",
    "   The training process is managed within the `optimize` method:\n",
    "   - **Forward Pass**: The generator produces the fake colorization output, which is stored in the `fake_color` variable.\n",
    "   - **Training the Discriminator**:  \n",
    "     - Fake images from the generator are passed to the discriminator, detached from the generator's computation graph, and labeled as \"fake.\"\n",
    "     - Real images from the training set are labeled as \"real.\"\n",
    "     - Both losses (for real and fake inputs) are computed, averaged, and used to update the discriminator weights.\n",
    "   - **Training the Generator**:  \n",
    "     - Fake images are passed to the discriminator, and the generator tries to fool the discriminator by assigning them \"real\" labels.\n",
    "     - The adversarial loss is combined with the L1 loss, which calculates the difference between the predicted and true color channels. The L1 loss is scaled by a coefficient (100 in this case) to balance the two losses.\n",
    "     - The combined loss is used to update the generator weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4, \n",
    "                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.lambda_L1 = lambda_L1\n",
    "        \n",
    "        if net_G is None:\n",
    "            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n",
    "        else:\n",
    "            self.net_G = net_G.to(self.device)\n",
    "        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
    "        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n",
    "        self.L1criterion = nn.L1Loss()\n",
    "        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
    "        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
    "    \n",
    "    def set_requires_grad(self, model, requires_grad=True):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "        \n",
    "    def setup_input(self, data):\n",
    "        self.L = data['L'].to(self.device)\n",
    "        self.ab = data['ab'].to(self.device)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.fake_color = self.net_G(self.L)\n",
    "    \n",
    "    def backward_D(self):\n",
    "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image.detach())\n",
    "        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n",
    "        real_image = torch.cat([self.L, self.ab], dim=1)\n",
    "        real_preds = self.net_D(real_image)\n",
    "        self.loss_D_real = self.GANcriterion(real_preds, True)\n",
    "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "        self.loss_D.backward()\n",
    "    \n",
    "    def backward_G(self):\n",
    "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image)\n",
    "        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n",
    "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n",
    "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
    "        self.loss_G.backward()\n",
    "    \n",
    "    def optimize(self):\n",
    "        self.forward()\n",
    "        self.net_D.train()\n",
    "        self.set_requires_grad(self.net_D, True)\n",
    "        self.opt_D.zero_grad()\n",
    "        self.backward_D()\n",
    "        self.opt_D.step()\n",
    "        \n",
    "        self.net_G.train()\n",
    "        self.set_requires_grad(self.net_D, False)\n",
    "        self.opt_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.opt_G.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.xx Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions were nor included in the explanations of the TDS article. These are just some utility functions to log the losses of our network and also visualize the results during training. So here you can check them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.count, self.avg, self.sum = [0.] * 3\n",
    "    \n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += count * val\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def create_loss_meters():\n",
    "    loss_D_fake = AverageMeter()\n",
    "    loss_D_real = AverageMeter()\n",
    "    loss_D = AverageMeter()\n",
    "    loss_G_GAN = AverageMeter()\n",
    "    loss_G_L1 = AverageMeter()\n",
    "    loss_G = AverageMeter()\n",
    "    \n",
    "    return {'loss_D_fake': loss_D_fake,\n",
    "            'loss_D_real': loss_D_real,\n",
    "            'loss_D': loss_D,\n",
    "            'loss_G_GAN': loss_G_GAN,\n",
    "            'loss_G_L1': loss_G_L1,\n",
    "            'loss_G': loss_G}\n",
    "\n",
    "def update_losses(model, loss_meter_dict, count):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        loss = getattr(model, loss_name)\n",
    "        loss_meter.update(loss.item(), count=count)\n",
    "\n",
    "def lab_to_rgb(L, ab):\n",
    "    \"\"\"\n",
    "    Takes a batch of images\n",
    "    \"\"\"\n",
    "    \n",
    "    L = (L + 1.) * 50.\n",
    "    ab = ab * 110.\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    rgb_imgs = []\n",
    "    for img in Lab:\n",
    "        img_rgb = lab2rgb(img)\n",
    "        rgb_imgs.append(img_rgb)\n",
    "    return np.stack(rgb_imgs, axis=0)\n",
    "    \n",
    "def visualize(model, data, save=True):\n",
    "    model.net_G.eval()\n",
    "    with torch.no_grad():\n",
    "        model.setup_input(data)\n",
    "        model.forward()\n",
    "    model.net_G.train()\n",
    "    fake_color = model.fake_color.detach()\n",
    "    real_color = model.ab\n",
    "    L = model.L\n",
    "    fake_imgs = lab_to_rgb(L, fake_color)\n",
    "    real_imgs = lab_to_rgb(L, real_color)\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    for i in range(5):\n",
    "        ax = plt.subplot(3, 5, i + 1)\n",
    "        ax.imshow(L[i][0].cpu(), cmap='gray')\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 5)\n",
    "        ax.imshow(fake_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 10)\n",
    "        ax.imshow(real_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "    if save:\n",
    "        fig.savefig(f\"colorization_{time.time()}.png\")\n",
    "        \n",
    "def log_results(loss_meter_dict):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        print(f\"{loss_name}: {loss_meter.avg:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7- Training¬†function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe this code is self-explanatory. Each epoch takes approximately 4 minutes on a moderately powerful GPU, such as the Nvidia P5000. If a more powerful GPU, like the 1080Ti or higher, is used, the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, epochs, display_every=200):\n",
    "    data = next(iter(val_dl)) # getting a batch for visualizing the model output after fixed intrvals\n",
    "    for e in range(epochs):\n",
    "        loss_meter_dict = create_loss_meters() # function returing a dictionary of objects to \n",
    "        i = 0                                  # log the losses of the complete network\n",
    "        for data in tqdm(train_dl):\n",
    "            model.setup_input(data) \n",
    "            model.optimize()\n",
    "            update_losses(model, loss_meter_dict, count=data['L'].size(0)) # function updating the log objects\n",
    "            i += 1\n",
    "            if i % display_every == 0:\n",
    "                print(f\"\\nEpoch {e+1}/{epochs}\")\n",
    "                print(f\"Iteration {i}/{len(train_dl)}\")\n",
    "                log_results(loss_meter_dict) # function to print out the losses\n",
    "                visualize(model, data, save=False) # function displaying the model's outputs\n",
    "\n",
    "model = MainModel()\n",
    "train_model(model, train_dl, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each epoch takes approximately 3 to 4 minutes on Colab. After around 20 epochs, some reasonable results should begin to appear.\n",
    "\n",
    "To further evaluate the model, it was trained for a longer duration (about 100 epochs). Below are the results of the baseline model:\n",
    "\n",
    "**Baseline Results:**\n",
    "\n",
    "The baseline model demonstrates a basic understanding of common objects in images, such as the sky and trees. However, its output remains far from satisfactory:\n",
    "- It struggles to decide on the color of less common or complex objects.\n",
    "- The outputs exhibit noticeable color spillovers and circular color artifacts (e.g., the center of the first image in the second row), which detract from the overall quality.\n",
    "\n",
    "These limitations suggest that with this small dataset, the current strategy is insufficient for achieving high-quality results. As a result, a different approach is required to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result\n",
    "\n",
    "Below are the results of the trained model, showcasing the **before** (grayscale input) and **after** (colorized output) images. The model has successfully added color to the grayscale images, demonstrating its ability to learn and apply colorization effectively.\n",
    "\n",
    "### Before and After\n",
    "\n",
    "#### Before:\n",
    "<img src=\"sample.jpg\" alt=\"Grayscale Image 1\" width=\"256\" height=\"176\"/>  \n",
    "<img src=\"sample_1.jpg\" alt=\"Grayscale Image 2\" width=\"256\" height=\"176\"/>  \n",
    "<img src=\"sample_2.jpg\" alt=\"Grayscale Image 3\" width=\"256\" height=\"176\"/>\n",
    "\n",
    "#### After:\n",
    "<img src=\"colorized_sample.png\" alt=\"Colorized Image 1\" width=\"256\" height=\"176\"/>  \n",
    "<img src=\"colorized_sample_1.png\" alt=\"Colorized Image 2\" width=\"256\" height=\"176\"/>  \n",
    "<img src=\"colorized_sample_2.png\" alt=\"Colorized Image 3\" width=\"256\" height=\"176\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "These results highlight the model's potential for enhancing grayscale images by adding realistic colors. While some improvements could be made for rare or complex objects, the overall performance is promising.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
